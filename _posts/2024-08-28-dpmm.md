---
layout: article
title: Dirichlet Process Mixture Models 
mathjax: true
tags: nonpar 
---

## Introduction to Finite Mixtures
Data is often a heterogeneous mix of different types of groups. In such scenarios, we need variable(s) to identify the group or category to which the data point belongs. Finite mixture models are a popular choice to model data with known/unknown group structures. If the group structure is known, we have a classification problem while an unknown group structure points to a clustering problem. Generally, finite mixtures model data to be coming from a known/unknown number of parametric densities. This modeling assumption makes finite mixture models an intuitive choice for density estimation as well. For instance, skewed or heavy-tailed histograms might be better approximated using finite mixtures than symmetric distributions like the normal distribution. Moreover, finite mixture models have theoretical guarantees to justify their usefulness. One of the most recent theoretical guarantees I found is by Nguyen et al[^2]. The main theorem Nguyen et al proved is given below. For rigorous mathematical details, refer to Theorem 2.1 in the article[^2].

#### Theorem
Let $f$ and $g$ be _continuous_ probability densities on $\mathbb{R}^n$ i.e. $f,g: \mathbb{R}^n \longrightarrow \mathbb{R}$. Define the set of $m$-component location-scale finite mixture of the PDF $g$ as follows:

$$
\mathcal{M}^g_m = \left \lbrace h^g_m \quad : \quad h^g_m(x) \triangleq \sum_{i=1}^m c_i \frac{1}{\sigma_i^n} g\left(\frac{x - \mu_i}{\sigma_i}\right) \text{ such that } \mu_i \in \mathbb{R}^n, \sigma_i > 0 \forall i \text{ and } \sum_{j=1}^m c_i = 1 \right \rbrace
$$

Further, let $\mathcal{M}^g \coloneqq \bigcup_{m \in \mathbb{N}} \mathcal{M}^g_m$. Then the following statements are true:
1. Let $\mathbb{K} \subset \mathbb{R}^n$ be a compact set. Then there exists a sequence of functions $\left \lbrace h_{m}^g \right \rbrace_{m \in \mathbb{N}}$ such that $h_{m}^g \in \mathcal{M}^g$ for each $g \in \mathbb{N}$ and

$$
\lim_{m\to\infty} \parallel f - h_m^{g} \parallel_{\mathcal{B}(\mathbb{K})} = 0 \text{ where } \parallel f \parallel_{\mathcal{B}(\mathbb{K})} \coloneqq sup_{x \in \mathbb{K}} |f(x)| \text{ for bounded function } f:\mathbb{K} \longrightarrow \mathbb{R}
$$

2. For $p>1$, if $f \in \mathcal{L_p} \text{ and } g \in \mathcal{L_{\infty}}$, then there exists a sequence of functions $\left \lbrace h_{m}^g \right \rbrace_{m \in \mathbb{N}}$ such that $h_{m}^g \in \mathcal{M}^g$ for each $g \in \mathbb{N}$ and

$$
\lim_{m\to\infty} \parallel f - h_m^{g} \parallel_{\mathcal{L_p}} = 0 \text{ where } \parallel f \parallel_{\mathcal{L_p}} \coloneqq \left( \int_{\mathbb{X}} | f|^p \mathrm{d} \lambda  \right )^{\frac{1}{p}} \text{ for Lebesgue measure } \lambda \text{ on } \mathbb{R}^n
$$
## Bayesian non-parametric Statistics



## Bayesian non-parametric mixtures























### References
1. McLachlan, G., and Peel, D. (2000), “_Finite Mixture Models_,” Wiley Series in Probability and Statistics, Wiley. DOI: 10.1002/0471721182
2. Nguyen, T., Chamroukhi, F., Nguyen, H. D., and McLachlan, G. J. (2022), “_Approximation of probability density functions via location-scale finite mixtures in Lebesgue spaces_,” Communications in Statistics - Theory and Methods, Informa UK Limited. DOI: 10.1080/03610926.2021.2002360.
