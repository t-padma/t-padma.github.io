---
layout: article
title: Dirichlet Process Mixture Models 
mathjax: true
tags: nonpar 
---

## Introduction to Finite Mixtures
Data is often a heterogeneous mix of different types of groups. In such scenarios, we need variable(s) to identify the group or category to which the data point belongs. Finite mixture models are a popular choice to model data with known/unknown group structures. If the group structure is known, we have a classification problem while an unknown group structure points to a clustering problem. Generally, finite mixtures model data to be coming from a known/unknown number of parametric densities. This modeling assumption makes finite mixture models an intuitive choice for density estimation as well. For instance, skewed or heavy-tailed histograms might be better approximated using finite mixtures than symmetric distributions like the normal distribution.

Karl Pearson, in 1894, was among the first to employ finite mixture models to analyze crab data provided by the evolutionary biologist Weldon. The crab data consisted of measurements of the forehead to body length of a thousand crabs sampled from the Bay of Naples. Each of these measurements falls into one of 29 interval categories. This dataset is saved as the dataframe `pearson` in the R Package `mixdist`. Look at the link[^3] to learn more about the data analysis. Pearson assumed that forehead-to-body length ratios are sampled from a mixture of two normal densities with different means and variances. Pearson used the method of moments to estimate the mean and variance parameters. This involved solving a ninth-degree polynomial by hand. Clearly, due to a lack of computational power, finite mixture models weren't studied as much until the advent of computers. The advent of computers and a better understanding of the properties of the maximum likelihood function in the case of normal components led to an eventual increase in the use of mixture models. However, it was the paper introducing the EM algorithm (1977) that greatly stimulated interest in using finite mixtures to model heterogeneous data.[^1]

### Mixture Model formulation
If we model the data $\mathbf{Y} \in \mathbb{R}^n$ is drawn from a mixture distribution containing $G$ parametric component densities $f(\cdot ; \theta_i)$ with proportions $\pi_i$, then

$$
\mathbf{Y} \sim \sum_{i=1}^G \pi_i f(\mathbf{y}; \theta_i) \quad \text{ where } \sum_{j=1}^G \pi_j = 1 \text{ and } G < \infty   \quad \quad ---(1)
$$

Equivalently, mixture models can be interpreted as the expectation with respect to a mixing measure $H$. Define $H$ to be a measure with finite support that has weight $\pi_j$ at position $\theta_j$. Note that  (1) and (2) are equivalent representations of a mixture model.

$$
\mathbf{Y} \sim \int_{\Theta} f(\cdot ; \theta ) H(\mathrm{d}\theta) \quad \quad ---(2)
$$

Moreover, finite mixture models have theoretical guarantees to justify their usefulness. One of the most recently proven theoretical guarantees I found is by Nguyen et al[^2]. The main theorem Nguyen et al proved is given below. For rigorous mathematical details, refer to Theorem 2.1 in the article[^2].

#### Theorem
Let $f$ and $g$ be _continuous_ probability densities on $\mathbb{R}^n$ i.e. $f,g: \mathbb{R}^n \longrightarrow \mathbb{R}$. Define the set of $m$-component location-scale finite mixture of the PDF $g$ as follows:

$$
\mathcal{M}^g_m = \left \lbrace h^g_m \quad : \quad h^g_m(x) \triangleq \sum_{i=1}^m c_i \frac{1}{\sigma_i^n} g\left(\frac{x - \mu_i}{\sigma_i}\right) \text{ such that } \mu_i \in \mathbb{R}^n, \sigma_i > 0 \forall i \text{ and } \sum_{j=1}^m c_i = 1 \right \rbrace
$$

Further, let $\mathcal{M}^g \coloneqq \bigcup_{m \in \mathbb{N}} \mathcal{M}^g_m$. Then the following statements are true:
1. Let $\mathbb{K} \subset \mathbb{R}^n$ be a compact set. Then there exists a sequence of functions $\left \lbrace h_{m}^g \right \rbrace_{m \in \mathbb{N}}$ such that $h_{m}^g \in \mathcal{M}^g$ for each $g \in \mathbb{N}$ and

$$
\lim_{m\to\infty} \parallel f - h_m^{g} \parallel_{\mathcal{B}(\mathbb{K})} = 0 \text{ where } \parallel f \parallel_{\mathcal{B}(\mathbb{K})} \coloneqq sup_{x \in \mathbb{K}} |f(x)| \text{ for bounded function } f:\mathbb{K} \longrightarrow \mathbb{R}
$$

2. For $p>1$, if $f \in \mathcal{L_p} \text{ and } g \in \mathcal{L_{\infty}}$, then there exists a sequence of functions $\left \lbrace h_{m}^g \right \rbrace_{m \in \mathbb{N}}$ such that $h_{m}^g \in \mathcal{M}^g$ for each $g \in \mathbb{N}$ and

$$
\lim_{m\to\infty} \parallel f - h_m^{g} \parallel_{\mathcal{L_p}} = 0 \text{ where } \parallel f \parallel_{\mathcal{L_p}} \coloneqq \left( \int_{\mathbb{X}} | f|^p \mathrm{d} \lambda  \right )^{\frac{1}{p}} \text{ for Lebesgue measure } \lambda \text{ on } \mathbb{R}^n
$$

Therefore, a mixture model can approximate complex densities arbitrarily well if we can choose the right component distributions. 



## Bayesian non-parametric Statistics



## Bayesian non-parametric mixtures























### References
1. McLachlan, G., and Peel, D. (2000), “_Finite Mixture Models_,” Wiley Series in Probability and Statistics, Wiley. DOI: 10.1002/0471721182
2. Nguyen, T., Chamroukhi, F., Nguyen, H. D., and McLachlan, G. J. (2022), “_Approximation of probability density functions via location-scale finite mixtures in Lebesgue spaces_,” Communications in Statistics - Theory and Methods, Informa UK Limited. DOI: 10.1080/03610926.2021.2002360.
3. [_Karl Pearson and the crabs from Naples bay_](https://rpubs.com/frapas/pearson_crabs), Francesco Pasqualini
